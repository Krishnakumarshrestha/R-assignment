---
title: "Multipale Regression"
author: "Krishna Kumar Shrestha"
date: "`r Sys.Date()`"
output: 
  html_document: 
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
library(tidyverse)
library(MASS)

```



# Excersis 21.1



**In the MASS package, you’ll find the data frame cats, which provides
data on sex, body weight (in kilograms), and heart weight (in grams)
for 144 household cats (see Venables and Ripley, 2002, for further
details); you can read the documentation with a call to ?cats. Load
the MASS package with a call to library("MASS"), and access the object
directly by entering cats at the console prompt.**


a. Plot heart weight on the vertical axis and body weight on the
horizontal axis, using different colors or point characters to
distinguish between male and female cats. Annotate your plot
with a legend and appropriate axis labels.


```{r}
data("cats")
cats %>%
  ggplot()+
  aes(x=Bwt,y=Hwt,col=Sex)+
  geom_point()+
  theme(
    legend.position = c(0.01,1),
    legend.justification = c("left", "top"),
    legend.box.just = "right",
    legend.margin = margin(6, 6, 6, 6)
    )
  
```


b. Fit a least-squares multiple linear regression model using heart
weight as the response variable and the other two variables as
predictors, and view a model summary.

```{r}
model1 <- lm(Hwt ~ Bwt + Sex, data = cats)
summary(model1)
```





i. Write down the equation for the fitted model and interpret
the estimated regression coefficients for body weight and
sex. Are both statistically significant? What does this say
about the relationship between the response and predictors?


>  the Equation is: y = -0.4149 + 4.0758*Bwt - 0.0821*SexM 

Ans:**From the summary table , we can see that p value of Bwt is less then 0.05 which means the variable is significant for the model where as the sex variable have p value more then 0.05, so the variable is insignificant for the model** 

ii. Report and interpret the coefficient of determination and
the outcome of the omnibus F -test.

Ans:**The coefficient of determination is 0.6468. It means that 65% of the variation of the Hwt can be explained by the model.even if it includes a not statistical relevant predictor variable in the model. **


c. Tilman’s cat, Sigma, is a 3.4 kg female. Use your model to estimate
her mean heart weight and provide a 95 percent prediction
interval.

```{r}

newdata = data.frame(Bwt = (3.4), Sex = ("F"))
predict(model1,
        newdata,
        interval = "prediction")
```



d. Use predict to superimpose continuous lines based on the fitted
linear model on your plot from (a), one for male cats and one
for female. What do you notice? Does this reflect the statistical
significance (or lack thereof) of the parameter estimates?



```{r}
cats %>%
  ggplot()+
  aes(x=Bwt,y=Hwt,col=Sex)+
  geom_point()+
  theme(
    legend.position = c(0.01,1),
    legend.justification = c("left", "top"),
    legend.box.just = "right",
    legend.margin = margin(6, 6, 6, 6)
    ) +
  geom_smooth(method = "lm",aes(col=Sex),se=0)
```






The boot package (Davison and Hinkley, 1997; Canty and Ripley,
2015) is another library of R code that’s included with the standard
installation but isn’t automatically loaded. Load boot with a call to
library("boot"). You’ll find a data frame called nuclear, which contains
data on the construction of nuclear power plants in the United States
in the late 1960s (Cox and Snell, 1981).
e. Access the documentation by entering ?nuclear at the prompt
and examine the details of the variables. (Note there is a mistake
for date, which provides the date that the construction permits
were issued—it should read “measured in years since January
1 1900 to the nearest month.”) Use pairs to produce a quick
scatterplot matrix of the data.

```{r}
library(boot)
data("nuclear")
pairs(nuclear)

```




f. One of the original objectives was to predict the cost of further
construction of these power plants. Create a fit and summary of
a linear regression model that aims to model cost by t1 and t2,
two variables that describe different elapsed times associated with
the application for and issue of various permits. Take note of the
estimated regression coefficients and their significance in the
fitted model.
```{r}
model2 <- lm(cost ~ t1 + t2, data = nuclear)
summary(model2)
```



g. Refit the model, but this time also include an effect for the date
the construction permit was issued. Contrast the output for this
new model against the previous one. What do you notice, and
what does this information suggest about the relationships in the
data with respect to these predictors?

```{r}
model3 <- lm(cost ~ t1 + t2 + date, data = nuclear)
summary(model3)
```


h. Fit a third model for power plant cost, using the predictors for
“date of permit issue,” “power plant capacity,” and the binary
variable describing whether the plant was sited in the northeastern
United States. Write down the fitted model equation
and provide 95 percent confidence intervals for each estimated
coefficient.

```{r}
model4 <- lm(cost ~ date + cap + ne, data = nuclear)
summary(model4)
#Equation is: -6458.3889006 +95.4385587*date + 0.4157157*cap + 126.1287688*ne
confint(model4)
```


The following table gives an excerpt of a historical data set compiled
between 1961 and 1973. It concerns the annual murder rate in
Detroit, Michigan; the data were originally presented and analyzed
by Fisher (1976) and are reproduced here from Harraway (1995).
In the data set you’ll find the number of murders, police officers,
and gun licenses issued per 100,000 population, as well as the overall
unemployment rate as a percentage of the overall population.









```{r echo=FALSE, warning=FALSE}
security<-data.frame(Murders=c(8.6,8.9,8.52,8.89,13.07,14.57,21.36,28.03,31.49,37.39,46.26,47.24,52.33),
                     Police=c(260.35,269.8,272.04,272.96,272.51,261.26,268.89,295.99,319.87,341.43,356.59,376.69,390.19),
                     Unemployment=c(11,7,5.2,4.3,3.5,3.2,4.1,3.9,3.6,7.1,8.4,7.7,6.3),
                     Guns=c(178.15,156.41,198.02,222.10,301.92,391.22,665.56,1131.21,837.6,794.9,817.74,583.17,709.59))
kableExtra::kable(security)
```

i. Create your own data frame in your R workspace and produce
a scatterplot matrix. Which of the variables appears to be most
strongly related to the murder rate?

```{r}
pairs(security)
```



j. Fit a multiple linear regression model using the number of
murders as the response and all other variables as predictors.
Write down the model equation and interpret the coefficients. Is
it reasonable to state that all relationships between the response
and the predictors are causal?


```{r}
model5<-lm(Murders~Police+Unemployment+Guns,data=security)
summary(model5)
#It is not reasonable to state that all relationshipps between response and 
#predictors are casual. We can see that Guns also presents a strong statistical 
# evidence of relationship with murders.
```



k. Identify the amount of variation in the response attributed to
the joint effect of the three explanatory variables. Then refit the
model excluding the predictor associated with the largest (in
other words, “most nonsignificant”) p-value. Compare the new
coefficient of determination with that of the previous model. Is
there much difference?


```{r}
model<-lm(Murders~Police+Guns,data=security)
summary(model)
#Removing the unemployment predictor variable there is a much better statistical
#relation between the predictors and the answer. But hte model keeps having a 
#great prediction capability.
```



l. Use your model from (k) to predict the mean number of murders
per 100,000 residents, with 300 police officers and 500
issued gun licenses. Compare this to the mean response if there
were no gun licenses issued and provide 99 percent confidence
intervals for both predictions.




```{r}
newdata = data.frame(Police = 300, Guns = c(500, 0))
predict.lm(
      model,
      newdata = newdata,
      interval = "confidence",
      level = 0.99
)
```






# Exercise 21.2


The following table presents data collected in one of Galileo’s
famous “ball” experiments, in which he rolled a ball down a ramp
of different heights and measured how far it traveled from the
base of the ramp. For more on this and other interesting examples,
look at “Teaching Statistics with Data of Historic Significance” by
Dickey and Arnold (1995).

a. Create a data frame in R based on this table and plot the data
points with distance on the y-axis.


```{r}
Galileu<-data.frame(Iniheight=c(1000,800,600,450,300,200,100),
                    Dist=c(573,534,495,451,395,337,257))

kableExtra::kable(Galileu)

Galileu %>%
  ggplot()+
  aes(x=Dist,y=Iniheight)+
  geom_point()+
  ggtitle("Distance Traveled Vs. Initial Height")+
  xlab("Initial Height")+
  ylab("Distance")

```




b. Galileo believed there was a quadratic relationship between
initial height and the distance traveled.
i. Fit an order 2 polynomial in height, with distance as the
response.

```{r}
m.2<-lm(Dist~Iniheight+I(Iniheight^2),data=Galileu)
summary(m.2)
```



ii. Fit a cubic (order 3) and a quartic (order 4) model for
these data. What do they tell you about the nature of the
relationship?


```{r}
m.3<-lm(Dist~Iniheight+I(Iniheight^2)+I(Iniheight^3),data=Galileu)
summary(m.3)
m.4<-lm(Dist~Iniheight+I(Iniheight^2)+I(Iniheight^3)+I(Iniheight^4),
             data=Galileu)
summary(m.4)
#Increasing the number of powers in the linear regression, we see that we have 
#an improovment in the prediction model, The model get to answer 99.99% of the
#response variable. altough, the new variables tend to loose statistical 
#relevance. For that, we would use the galfit.3
```



c. Based on your models from (b), choose the one that you think
best represents the data and plot the fitted line on the raw data.
Add 90 percent confidence bands for mean distance traveled to
the plot.

```{r}
Galileu %>%
  ggplot()+
  aes(x=Dist,y=Iniheight)+
  geom_point()+
  ggtitle("Distance Traveled Vs. Initial Height")+
  xlab("Initial Height")+
  ylab("Distance") +
  geom_smooth(method = "lm")
```




The contributed R package faraway contains a large number of data
sets that accompany a linear regression textbook by Faraway (2005).
Install the package and then call library("faraway") to load it. One of
the data sets is trees, which provides data on the dimensions of felled
trees of a certain type (see, for example, Atkinson, 1985).
d. Access the data object at the prompt and plot volume against
girth (the latter along the x-axis).

```{r}
library("faraway")
data("trees")

trees %>%
  ggplot()+
  aes(x=Girth,y=Volume)

```



e. Fit two models with Volume as the response: one quadratic model
in Girth and the other based on log transformations of both
Volume and Girth. Write down the model equations for each and
comment on the similarity (or difference) of the fits in terms of
the coefficient of determination and the omnibus F-test.

```{r}
tree.quad.fit<-lm(Volume~Girth+I(Girth^2),data=trees)
summary(tree.quad.fit) #"Mean volume" = 10.79 - 2.09*"girth" + 0.254*"girth^2"

tree.log.fit<-lm(log(Volume)~log(Girth),
                     data=trees)
summary(tree.log.fit)
#Model equation is -2.35332 +219997*log(Girth)

#The models have a very strong R-Squared value, with the prediction explaining 
#more than 95% of the variance. Girth appears to be a good predictor for volume.
```



f. Use predict to add lines to the plot from (d) for each of the two
models from (e). Use different line types; add a corresponding
legend. Also include 95 percent prediction intervals, with line
types matching those of the fitted values (note that for the model
that involves log transformation of the response and the predictor,
any returned values from predict will themselves be on the
log scale; you have to back-transform these to the original scale



```{r}
tree.seq<-seq(min(trees[,1]),max(trees[,1]),length.out = 100)

tree.quad.pred<-predict(tree.quad.fit,
                        newdata = data.frame(Girth=tree.seq),
                        interval="predict")
tree.log.pred <- predict(tree.log.fit,
                         newdata = data.frame(Girth = tree.seq),
                         interval = "predict")

plot(Volume~Girth,data=trees)

lines(tree.seq,tree.quad.pred[,1],lwd=2,col="blue",lty=3)
lines(tree.seq,tree.quad.pred[,2],lwd=0.5,col="blue",lty=3)
lines(tree.seq,tree.quad.pred[,3],lwd=0.5,col="blue",lty=3)

lines(tree.seq,exp(tree.log.pred[,1]),lwd=2,col="green",lty=4)
lines(tree.seq,exp(tree.log.pred[,2]),lwd=0.5,col="green",lty=4)
lines(tree.seq,exp(tree.log.pred[,3]),lwd=0.5,col="green",lty=4)
#Although the models themselves are very similar, confidence interval for the 
#log one is much wider than the quadratic, for higher values. To define the best
#model, more information will be needed.
```







g. Fit and summarize a multiple linear regression model to determine
mean MPG from horsepower, weight, and displacement.



```{r}

carsfit<-lm(mpg~hp+wt+disp,data=mtcars)
summary(carsfit)
```

h. In the spirit of Henderson and Velleman (1981), use I to refit
the model in (g) in terms of GPM = 1=MPG. Which model
explains a greater amount of variation in the response?


```{r}
carsfit2<-lm(I(1/mpg)~hp+wt+disp,data=mtcars)
summary(carsfit2)
#the model that explains a great amount of the variation in this example is the 
#one with the transformation of the variable. as it's R-Squared presents that 
#85% of the variance is explaines by the model. Instead of the 83% previously
# achieved.
```




# Excersise 21.3


```{r}
#First Part ----
library("MASS")
?cats

#a ----
cat.original.fit<-lm(Hwt~Bwt+Sex,data=cats)
summary(cat.original.fit)
cat.fit<-lm(Hwt~Bwt*Sex,data=cats)
summary(cat.fit)

#The significance of the Bwt had decreased, ut the remaining variables 
#significance had increased. ALso, Bwt Main interaction had decreased compared
# with the new model with interactive interactions. Also, being a male had a 
#slight positive interative effect on Bwt.

#b ----
plot(cats$Hwt~cats$Bwt,
     col=c("red","blue")[cats$Sex],
     xlab="Body Weight",
     ylab="Heart Weight",
     main="Cat's Body Weight by Heart Weight")
legend("topleft",
       legend=c("Female","male"),
       fill=c("red","blue"))

#Define the coeficients
cat.male<-c(coef(cat.fit)[1]+coef(cat.fit)[3],coef(cat.fit)[2]*coef(cat.fit)[4])
cat.fema<-c(coef(cat.fit)[1],coef(cat.fit)[2])
abline(cat.male,col="blue")
abline(cat.fema,col="red")

#Now, the interactive interactions are impacting each other, being a Female 
#or being a Male have impact on the regressions HeartWeight. Previously, 
#using only the Main effects, no great difference between sex was identified.

#c ----
predict(cat.fit, #interactive predict
        newdata = data.frame(Bwt=3.4,Sex="F"),
        interval="prediction")
predict(cat.original.fit,#main predict
                      newdata = data.frame(Bwt=3.4,Sex="F"),
                      interval="prediction")

#The new model, with interactive effects can show the impact of being a 
#female has in the overall weight, bringing a lighter weight than predicted 
#previously

#Second Part ----

#d ----
library(faraway)
?trees

tree.mainfit<-lm(Volume~Girth+Height,data=trees)
summary(tree.mainfit)
tree.intfit<-lm(Volume~Girth*Height,data=trees)
summary(tree.intfit)

#The new model has a greater R-Squared value altough have a greater pvalue for
#the variables. Still, statistically acceptable 

#e ----
tree.log.mainfit<-lm(log(Volume)~log(Girth)+log(Height),data=trees)
summary(tree.log.mainfit)
tree.log.intfit<-lm(log(Volume)~log(Girth)*log(Height),data=trees)
summary(tree.log.intfit)
#we got an even greater Rsquared value, but now, all coefficients have huge
# significane levels, not acceptable. We cannot assume they have a log
# relation

#Third Part ----
#f ----
?mtcars
cars.fit<-lm(mpg~hp*factor(cyl)+wt,data=mtcars)
summary(cars.fit)

#g ----
#The hp variable is a continuous and the factor(cyl) is a categorical. Each of 
#'their interactions can be identified as a change in the slope of the 
#'model.

#h ----
#mom wants mpg>=25 
mycars<-data.frame(cyl=c(4,8,6),
                   hp=c(100,210,200),
                   wt=c(2.1,3.9,2.0),
                   row.names = c("Car1","Car2","Car3"))

predict(cars.fit,newdata = mycars,interval="confidence")

#i
#My predictive model suggest that the only car to achieve the desired MPG is
#car1

#ii 
#The regression includes for the car3 a upper limit that includes 25Mpg. 
#There is 95% of confidence the average for that car is between those values
```

